* Goal of proposed script is to check data consistency on all Tor network exit nodes.
* It checks only unencrypted traffic on exit nodes (so not HTTPS, FTPS, SSH etc),
* cause on encrypted traffic, there is no (real) possibility to modify content.
* Because script infrastructure is pluggable, there should be easy to extend
* functionality by adding plugin for new protocol.
*
* Author of this proposal: slush, Léta Páně 2008

Algorithms:
===========

first phase - preparation:
--------------------------
* Overview: In this step, script ask each plugin for internet resource,
* which is able to parse/check (html page, ftp file, ...).
* After that, web spider will start searching for these resources on
* Internet (Using Google as start point).
* Spider will stop after reaching limits on links per site, max files or bytes
* downloaded per rule etc.
* Vocabulary:
* Link - address to any internet resource
* Link bucket - queue of links to download by spider
* File - downloaded resource
* Content - content of file
* Whitelist
* Checklist - list of links to download thru Tor nodes
load web spider conf (max links, max links per site, max time, max bytes)
load whitelist (from file, from conf, ...)
checklist = []
add links from whitelist to link bucket
while link in bucket:
	select link from bucket (queue)
	if exceed "links per site" for this site:
		remove link from bucket
		skip to next link
	if resource seems to be any HTTP content (protocol?):
		download HEADERS of resource
		add headers to cache (for execution phase)
		if resource is html page (Content-Type):
			download file and parse content (TODO)
			get links from page to link bucket
	else (not HTTP content):
		headers = empty
	for each plugin:
		skip plugin, if its number of files or bytes reached limits
		if link matches to plugin rules (protocol, url, headers)
			add link to checklist
	remove link from bucket
	if spider limits reached (max time, bytes), break
	if limits on every plugins reached, break
save checklist

second phase - execution:
-------------------------
load conf (max time, max bytes)
load checklist
load plugins
connect to tor controller
connect to tor socks proxy
get list of all routers
exits = select nodes with flags: Exit, Running (?), Valid (?)
relays = select nodes with flags: Running, Uptime > 1 day
for each link in checklist:
	if resource seems to be any HTTP content (protocol?):
		download HEADERS of resource
for each exit node:
	while not valid circuit or tries > 3:
		create 2node circuit "random relay, exit node"
	nodebucket = checklist
	while link in nodebucket:
		for each plugin:
			if link matches to plugin rules (protocol, url, headers):
				call plugin._analyze(protocol, url, headers)
					v kodu zrusit whitespaces
					neukladat dedicnost bloku
					hashe v seznamu unikatni
					rekurzivni algoritmus?
				save to struct [link][exitnode]
	if spider limits reached (max time, bytes), break
	if limits on every plugins reached, break
		
third phase - analysis:
-----------------------
* Overview: Target of this phase is to cross-check plugin results of each matched file
* across exit nodes. That means that there is no sharp border between valid and invalid exit node,
* but some probability level of failure. When 99% exit nodes returns the same
* content of page and the last one returns something modified, there is probably problem.
* On other side, when only 20% nodes returns the same content, there is big probability, that
* "problem" is on content provider side (dynamic links, time-specific links etc).
* Each script user should consider weights of all aspect and select border level regards
* his own usage of Tor.
* Testing page: http://slush.cz/tortest.php

Vstup: seznam hashu pro kazdou stranku. Hashe delane z bloku bez whitespaces.
	Stejna stranka stazena pres vsechny exit nody, tj. X kopii teze stranky.
Vytvorim dlouhy seznam hashu jako unikatni hashe ze vsech kopii stranky.
Pro kazdy hash si spocitam pocet vyskytu v kopiich stranky.
Pro kazdy hash a kazdou stranku:
	Pokud hash na dane strance je:
		1 = hash se v ostatnich strankach nevyskytuje.
		0 = hash se vyskytuje ve vsech ostatnich strankach.
		res = 1 - (pocet ostatnich stranek, kde se hash vyskytuje / (pocet vsech stranek -1))
	Pokud hash na dane strance neni a mira vyskytu > 1:
		1 = hash se v ostatnich strankach vyskytuje.
		0 = hash se nevyskytuje na zadne dalsi strance.
		res = (pocet ostatnich stranek, kde se hash vyskytuje / (pocet vsech stranek -1))
	Pokud hash na dane strance neni a mira vyskytu == 1:
		res = 0
		# Hack pro silne dynamicke stranky, neni nutne.
Vysledkem je matice hashu a stranek, kde hodnotou je alert index.
Pro vsechny bloky s nizkym indexem spustit heuristicke testy nebezpecnych kodu.

Classes:
========
Protocol - implements top-level class for known internet services (HTTP, FTP, SSH)
<XXX>Protocol - has to contain method to retrieve protocol-specific content, eg. SSH.
